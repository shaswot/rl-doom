{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Doom - Basic scenario\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to set up our reinforcement learning environment with VizDoom and stable-baselines. This notebook has a corresponding [Medium article](https://lkieliger.medium.com/deep-reinforcement-learning-in-practice-by-playing-doom-part-1-getting-started-618c99075c77) which provides a high-level description.\n",
    "\n",
    "The first scenario that we will try to solve is called \"basic\". In this scenario, the player is located in a square room, facing an ennemy. Only three actions are available: shoot, move left or move right. At the beginning of each episode, the ennemy will be located at a random location along the opposite wall of the room. The ennemy is immobile and passive. The goal of the scenario is to shoot the ennemy as fast as possible. For every time step we receive a reward of -1, missing a shot rewards -5 and successfully shooting the ennemy provides a reward of 101. The scenario can last at most 300 steps after which it is automatically restarted. \n",
    "\n",
    "\n",
    "# Code\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 15:23:18.200356: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-22 15:23:18.235171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-22 15:23:18.754973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common import callbacks\n",
    "from stable_baselines3.common import evaluation\n",
    "from stable_baselines3.common import policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the learning task, we will need two components: the agent and the environment. Thanks to stable-baselines, building an agent will be very straightforward. The environment though requires some explanation. In the medium article we explain how to build the Gym environment wrapper for a VizDoom game instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import envs\n",
    "\n",
    "def create_env(**kwargs) -> envs.DoomEnv:\n",
    "    # Create a VizDoom instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config('scenarios/basic.cfg')\n",
    "    game.init()\n",
    "\n",
    "    # Wrap the environment with the Gym adapter.\n",
    "    return envs.DoomEnv(game, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one additional step required to make it work with stable-baselines. The good new is that this small additional step will unlock very cools features for the learning task.\n",
    "\n",
    "What we need to set up is a vectorized environment. Stable-baselines' [doc](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vectransposeimage) has a nice explanation on what you can do with vectorized environments. Technically, we could do whithout but since we'll be exploring training on multiple environment later anyway, let's go ahead and see how to create one. In PyTorch, convolutional layers expect as inputs an image of the shape (C, H, W) that it, channels first. We could ask VizDoom to directly render the frame buffer in [channel-first](https://github.com/mwydmuch/ViZDoom/blob/master/doc/Types.md#-screenformat) mode. However, as we will see below, it is a better idea to keep the image channel last as this will allow us to make some image preprocessing using OpenCV. Thankfully, stable-baselines offers a [VecTransposeImage](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vectransposeimage) wrapper whose sole purpose is to convert from channel last to channel first and vice-versa. As you can see from the function signature, VecTransposeImage requires a VecEnv to work. So let's make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "\n",
    "def create_vec_env(**kwargs) -> VecTransposeImage:\n",
    "    vec_env = DummyVecEnv([lambda: create_env(**kwargs)])\n",
    "    return VecTransposeImage(vec_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's unpack what's going on here. First we have create a dummy vectorized environment. It need a list of function that when called, will generate our environment. Since there is only one element in this list we will end up with only one environment. Learning with multiple environment later will be as simple as adding more elements to that list. Then, we pass this dummy vectorized environment to the image transposition wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although VizDoom provides many different screen resolutions. It is useful to define an additional frame processing step in which we rescale the input image by a given factor. This allows us the render the game in a nice resolution while keeping the input size to the model to an acceptable range. This can be easily done with a call to the resize function of OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_processor = lambda frame: cv2.resize(frame, None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so allow us to take advantage of OpenCV's interpolation to have a smoother image at the same resolution. A side-by-side comparison should make the advantage of rescaling the image ourselves pretty clear. Notice how aliased the image on the left is. This effect get worse and worse as the game tries to render furhter away (smaller) objects. If we can \"see\" better in the second image, so will our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "Could not initialize SDL video:\n",
      "No available video device\n",
      "\n"
     ]
    },
    {
     "ename": "ViZDoomErrorException",
     "evalue": "Could not initialize SDL video:\nNo available video device\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomErrorException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m f, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compare 160 by 120 resolution vs. 320 by 240 resolution, scaled by half using OpenCV.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sample_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvizdoom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScreenResolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRES_160X120\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(frame_processor(utils\u001b[38;5;241m.\u001b[39mget_sample_frame(vizdoom\u001b[38;5;241m.\u001b[39mScreenResolution\u001b[38;5;241m.\u001b[39mRES_320X240)))\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[0;32m/repos/rl-doom/standalone_examples/basic/utils.py:12\u001b[0m, in \u001b[0;36mget_sample_frame\u001b[0;34m(resolution)\u001b[0m\n\u001b[1;32m     10\u001b[0m game\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m game\u001b[38;5;241m.\u001b[39mset_screen_resolution(resolution)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Collect frame\u001b[39;00m\n\u001b[1;32m     15\u001b[0m frame \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mget_state()\u001b[38;5;241m.\u001b[39mscreen_buffer\n",
      "\u001b[0;31mViZDoomErrorException\u001b[0m: Could not initialize SDL video:\nNo available video device\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from basic import utils\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Compare 160 by 120 resolution vs. 320 by 240 resolution, scaled by half using OpenCV.\n",
    "ax[0].imshow(utils.get_sample_frame(vizdoom.ScreenResolution.RES_160X120))\n",
    "ax[1].imshow(frame_processor(utils.get_sample_frame(vizdoom.ScreenResolution.RES_320X240)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the agent and the environment\n",
    "\n",
    "We will create two environment, one for training and one for evaluatinng our agent. We will pass to both those environment a couple of parameters that are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "Could not initialize SDL video:\n",
      "No available video device\n",
      "\n"
     ]
    },
    {
     "ename": "ViZDoomErrorException",
     "evalue": "Could not initialize SDL video:\nNo available video device\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomErrorException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m env_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Number of steps for which the last action is repeated.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_skip\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Scaling the input image allows us to see the game in good resolution while \u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_processor\u001b[39m\u001b[38;5;124m'\u001b[39m: frame_processor\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create training and evaluation environments.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m training_env, eval_env \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vec_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_args\u001b[49m\u001b[43m)\u001b[49m, create_vec_env(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_args)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mcreate_vec_env\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vec_env\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecTransposeImage:\n\u001b[0;32m----> 4\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecTransposeImage(vec_env)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(fn()) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mcreate_vec_env.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vec_env\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecTransposeImage:\n\u001b[0;32m----> 4\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecTransposeImage(vec_env)\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mcreate_env\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m game \u001b[38;5;241m=\u001b[39m vizdoom\u001b[38;5;241m.\u001b[39mDoomGame()\n\u001b[1;32m      6\u001b[0m game\u001b[38;5;241m.\u001b[39mload_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscenarios/basic.cfg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Wrap the environment with the Gym adapter.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mDoomEnv(game, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mViZDoomErrorException\u001b[0m: Could not initialize SDL video:\nNo available video device\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    # Number of steps for which the last action is repeated.\n",
    "    'frame_skip': 4,\n",
    "    # Scaling the input image allows us to see the game in good resolution while \n",
    "    'frame_processor': frame_processor\n",
    "}\n",
    "\n",
    "# Create training and evaluation environments.\n",
    "training_env, eval_env = create_vec_env(**env_args), create_vec_env(**env_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, with stable baselines creating the agent is really straightforward. We can leave most of the parameters to their default values. By calling the PPO constructor we will get an agent with an actor-critic policy with the following structure in five parts:\n",
    "\n",
    "```\n",
    "       features extractor\n",
    "               |\n",
    "          mlp extractor\n",
    "               |\n",
    "            features\n",
    "            /      \\\n",
    "           /        \\\n",
    "   action net      value net\n",
    "```\n",
    "If you want to know more about actor-critic methods, Rudy Gilman has an [awesome article](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752) vulgarizing the subject by means of a comic.\n",
    "\n",
    "By default, the feature extractor is the [NatureCNN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), the mlp extractor is empty and the action and value net are two sequential models mapping the features to the action and value output respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(env, **kwargs):\n",
    "    return ppo.PPO(policy=policies.ActorCriticCnnPolicy,\n",
    "                   env=env,\n",
    "                   n_steps=4096,\n",
    "                   batch_size=32,\n",
    "                   learning_rate=1e-4,\n",
    "                   tensorboard_log='logs/tensorboard',\n",
    "                   verbose=0,\n",
    "                   seed=0,\n",
    "                   **kwargs)\n",
    "\n",
    "agent = create_agent(training_env)\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we have everything we need to start the learning task! Within a couple of iterations you should see some progress in the behavior of the agent. Our model stabilizes at a total reward of around 85 which. The maximum score being 100, it means the model takes on average less than four steps to succeed (remember, we are repeated each action four actions due to the `frame_skip` parameter), that's awesome. Give yourself a tap on the back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define an evaluation callback that will save the model when a new reward record is reached.\n",
    "evaluation_callback = callbacks.EvalCallback(eval_env,\n",
    "                                             n_eval_episodes=10,\n",
    "                                             eval_freq=5000,\n",
    "                                             log_path='logs/evaluations/basic',\n",
    "                                             best_model_save_path='logs/models/basic')\n",
    "\n",
    "# Play!\n",
    "agent.learn(total_timesteps=40000, tb_log_name='ppo_basic', callback=evaluation_callback)\n",
    "\n",
    "training_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained agent as a gif\n",
    "\n",
    "Now that we have an agent able to align and shoot enemies, let's make a little souvenir and save it's performance in the form of an animated gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def make_gif(agent, file_path):\n",
    "    env = create_vec_env(frame_skip=1, frame_processor=frame_processor)\n",
    "    env.venv.envs[0].game.set_seed(0)\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    for i in range(20):\n",
    "        obs = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            images.append(env.venv.envs[0].game.get_state().screen_buffer)\n",
    "\n",
    "    imageio.mimsave(file_path, images, fps=35)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to pick the best model from a previous training session\n",
    "# agent = ppo.PPO.load('logs/models/basic/best_model.zip')\n",
    "make_gif(agent, 'figures/basic_agent.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the agent has the tendency to shoot too early in the gif. This is because to create a smooth-looking gif we had to create an environment where the frame skip is set to 1. We can fix this very easily by taking our trained agent and learning a few more steps but this time with the final environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_vec_env(frame_skip=1, frame_processor=frame_processor)\n",
    "\n",
    "agent.set_env(env)\n",
    "agent.learn(total_timesteps=20000)\n",
    "\n",
    "env.close()\n",
    "make_gif(agent, 'figures/basic_agent_adjusted.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison frame skip](figures/basic_agent_adjusted.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Improvements\n",
    "\n",
    "## Comparing the effects of the frame skip parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning models are notoriously hard to train. Often, a lot of care must be put into finding the correct hyperparameters for the model: learning rate, clip range, epochs per update etc. I found the frame skip setting to be one of the most influencial parameters on the learning performance. My intuitive explanation is that a larger frame skip allow the model to explore more rapidly the environment and thus encounter positive situation more often. Indeed if you try setting it to one for example, you will find that for many episodes where the monster is spawned near the border walls, the agent makes random action that tend to stay within the center of the room. This behaviour means that he is much less likely to shoot the ennemy by luck and there much less likely to receive positive rewards.\n",
    "\n",
    "I conducted a series of experiments with varying frame skip sizes, running 6 consecutive learning and averaged the results. Each error bar denotes the error mean: $ \\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "![Comparison frame skip](figures/basic_comparison_frame_skip.png)\n",
    "\n",
    "The second most influencial parameter on the learning performance is the learning rate. In most of the VizDoom examples, I found the maximum possible learning rate to be between 1e-3 and 1-4. Empirically, a learning rate that is too high seems to quickly push the agent in a state where it always outputs the same actions whereas a learning rate that is too low makes the learning procedure needlessly long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have realized that using 160 by 120 pixels input image for this scenario is completely unnecessary. The roof and the floors do not add any meaningful information to our model, only the middle of the frame posesses information. Cutting the meaningless parts means that our model will need significantly less trainable parameters. At the moment, the number of parameters is:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    print('Number of trainable parameters: {:,}'.format(\n",
    "    sum(p.numel() for p in model.policy.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we modify our environment and use instead a cropped version of the input frame, we can reduce the number of parameters by 2.6M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_processor = lambda frame: cv2.resize(frame[40:-41, :], None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "env = create_vec_env(frame_processor=frame_processor); agent = create_agent(env); env.close()\n",
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the PPO class from stable-baselines allows us to directly modify the architecture of the model using keyword arguments. In particular, the default number of features produced at the end of the convolutional layers is 512. This is also unnecessarily large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(env, policy_kwargs={'features_extractor_kwargs': {'features_dim': 128}})\n",
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we decreased the size of trainable parameters by a factor 7. Let see how this new model compares to the old. Training this simplified model produces results on par with the more complex one while being faster to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {'frame_skip': 4, 'frame_processor': frame_processor}\n",
    "\n",
    "training_env, eval_env = create_vec_env(**env_args), create_vec_env(**env_args)\n",
    "agent.set_env(training_env)\n",
    "\n",
    "evaluation_callback = callbacks.EvalCallback(eval_env,\n",
    "                                             n_eval_episodes=10,\n",
    "                                             eval_freq=5000,\n",
    "                                             log_path='logs/evaluations/basic',\n",
    "                                             best_model_save_path='logs/models/basic')\n",
    "\n",
    "agent.learn(total_timesteps=40000, tb_log_name='ppo_basic_simplified', callback=evaluation_callback)\n",
    "\n",
    "training_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we've seen how to setup and adapt VizDoom to use it as reinforcement learning environment with the powerful stable-baselines library. In the next chapter we will explore a more complex scenario and dwelve into the model in more details. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
